{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["D0ikS0Sf_GFm","gaqifUvpLxMk","QuxBiIDsUsNG"],"gpuType":"T4","mount_file_id":"1k5cGzxCvSom0mOjunshEeK_FL7_3ZyNc","authorship_tag":"ABX9TyOQobe0xjGECJZ/ptx7WT28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**FrozenLakeDQN**\n","\n","**Description:**  \n","FrozenLakeDQN is a reinforcement learning project that uses **Deep Q-Networks (DQN)** to train an agent to solve the **FrozenLake-v1** environment. The agent learns to navigate a slippery 4x4 gridworld, avoiding holes and reaching the goal.  \n","\n","**Technologies Used:**  \n","- **TensorFlow** – For building and training the neural network.  \n","- **NumPy** – For handling data and state representations.  \n","- **Gymnasium** – For environment simulation and interaction.  \n","\n","**How It Works:**  \n","1. The agent uses **one-hot encoding** to represent states.  \n","2. A **neural network** predicts Q-values for actions.  \n","3. The agent **explores** (random actions) and **exploits** (best actions) to improve over time.  \n","4. **Experience replay** and a **target network** stabilize training.  \n","5. The model learns using **reward-based feedback** to maximize long-term success.\n","\n"],"metadata":{"id":"D0ikS0Sf_GFm"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import time\n","from IPython.display import clear_output\n","\n","# تحميل البيئة\n","env = gym.make('FrozenLake-v1', render_mode=None, is_slippery=True)\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n"],"metadata":{"id":"OeuSjx-WAZmr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# إنشاء النموذج\n","def build_model():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=(num_states,)),\n","        tf.keras.layers.Dense(64, activation='relu'),\n","        tf.keras.layers.Dense(64, activation='relu'),\n","        tf.keras.layers.Dense(num_actions)  # Q-values لكل أكشن\n","    ])\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n","    return model"],"metadata":{"id":"y0ckVqnZAeRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تحويل الحالة إلى One-hot\n","def one_hot_state(state):\n","    state_one_hot = np.zeros(num_states)\n","    state_one_hot[state] = 1\n","    return state_one_hot.reshape(1, -1)\n","\n","# اختيار الأكشن بناءً على السياسة ε-greedy\n","def select_action(model, state, epsilon):\n","    if np.random.rand() <= epsilon:\n","        return env.action_space.sample()  # اختيار عشوائي\n","    q_values = model.predict(state, verbose=0)\n","    return np.argmax(q_values[0])\n"],"metadata":{"id":"1Zq7z9qkAfRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تحديث Q-values باستخدام تجربة إعادة التشغيل\n","def update_q_values(model, target_model, batch, gamma=0.99):\n","    states, actions, rewards, next_states, dones = zip(*batch)\n","\n","    states = np.array([s[0] for s in states])\n","    next_states = np.array([s[0] for s in next_states])\n","\n","    q_values = model.predict(states, verbose=0)\n","    q_next = target_model.predict(next_states, verbose=0)\n","\n","    for i in range(len(batch)):\n","        if dones[i]:\n","            q_values[i][actions[i]] = rewards[i]\n","        else:\n","            q_values[i][actions[i]] = rewards[i] + gamma * np.max(q_next[i])\n","\n","    model.train_on_batch(states, q_values)\n"],"metadata":{"id":"wK4dSwI2AfaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# دالة التدريب\n","def train_agent(num_episodes=1000, target_update_freq=10, batch_size=32):\n","    model = build_model()\n","    target_model = build_model()\n","    target_model.set_weights(model.get_weights())\n","\n","    epsilon = 1.0\n","    epsilon_min = 0.01\n","    epsilon_decay = 0.995\n","    memory = []\n","    max_memory_size = 2000\n","\n","    rewards_list = []\n","\n","    for episode in range(num_episodes):\n","        state = env.reset()[0]\n","        state = one_hot_state(state)\n","        done = False\n","        total_reward = 0\n","\n","        while not done:\n","            action = select_action(model, state, epsilon)\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            next_state = one_hot_state(next_state)\n","\n","            if done and not terminated:\n","                reward = -1  # إذا لم يصل للنهاية بعد عدد خطوات معين\n","\n","            memory.append((state, action, reward, next_state, done))\n","            if len(memory) > max_memory_size:\n","                memory.pop(0)\n","\n","            state = next_state\n","            total_reward += reward\n","\n","            if len(memory) >= batch_size:\n","                batch_indices = np.random.choice(len(memory), batch_size, replace=False)\n","                batch = [memory[i] for i in batch_indices]\n","                update_q_values(model, target_model, batch)\n","\n","        rewards_list.append(total_reward)\n","\n","        if episode % target_update_freq == 0:\n","            target_model.set_weights(model.get_weights())\n","\n","        if epsilon > epsilon_min:\n","            epsilon *= epsilon_decay\n","\n","        if episode % 50 == 0:\n","            clear_output(wait=True)\n","            print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n","\n","    return model"],"metadata":{"id":"-wFsZ13mAv-X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تدريب الوكيل\n","trained_model = train_agent(num_episodes=500)\n","\n","# حفظ الموديل\n","trained_model.save(\"/content/drive/MyDrive/Hub/Semester Winter 2025/PROJ-DRL/frozen_lake_dqn.h5\")\n","print(\"Model saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wD_mVPM2A0Dx","executionInfo":{"status":"ok","timestamp":1742316078563,"user_tz":-120,"elapsed":1938655,"user":{"displayName":"Karim Mansor","userId":"09451720980622133913"}},"outputId":"054afa8c-e154-4e2b-9a09-bc503eff0af0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 450, Reward: 0.0, Epsilon: 0.1043\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model saved successfully.\n"]}]},{"cell_type":"markdown","source":["# **Evolution**  Cont..."],"metadata":{"id":"gaqifUvpLxMk"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/Hub/Semester Winter 2025/PROJ-DRL/frozen_lake_dqn.h5\"\n","loaded_model = tf.keras.models.load_model(model_path, custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eIBsb1CK-un","executionInfo":{"status":"ok","timestamp":1742316861849,"user_tz":-120,"elapsed":44,"user":{"displayName":"Karim Mansor","userId":"09451720980622133913"}},"outputId":"b4e5ace6-d80d-452d-807b-38eb5a42be73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["def evaluate_agent(agent, env, num_episodes=100):\n","    success_count = 0\n","    total_steps = 0\n","\n","    for episode in range(num_episodes):\n","        state = env.reset()[0]\n","        state = one_hot_state(state)\n","        done = False\n","        steps = 0\n","\n","        while not done:\n","            q_values = agent.predict(state, verbose=0)\n","            action = np.argmax(q_values[0])\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            state = one_hot_state(next_state)\n","            steps += 1\n","\n","            if terminated and reward > 0:\n","                success_count += 1\n","\n","        total_steps += steps\n","\n","    success_rate = (success_count / num_episodes) * 100\n","    avg_steps = total_steps / num_episodes\n","\n","    print(f\"Success Rate: {success_rate:.2f}%\")\n","    print(f\"Average Steps to Goal: {avg_steps:.2f}\")\n","    return success_rate, avg_steps\n","\n","# Run evaluation\n","evaluate_agent(loaded_model, env)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TnUKcqMKy0v","executionInfo":{"status":"ok","timestamp":1742317113956,"user_tz":-120,"elapsed":246877,"user":{"displayName":"Karim Mansor","userId":"09451720980622133913"}},"outputId":"de11346a-0a3e-4326-9476-88e38f1e16a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Success Rate: 52.00%\n","Average Steps to Goal: 30.78\n"]},{"output_type":"execute_result","data":{"text/plain":["(52.0, 30.78)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["loaded_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"-2xc9epqNH8x","executionInfo":{"status":"ok","timestamp":1742317325298,"user_tz":-120,"elapsed":90,"user":{"displayName":"Karim Mansor","userId":"09451720980622133913"}},"outputId":"f7cfd733-e023-4500-81ed-81586121a1ce"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_8\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m1,088\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m260\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,510\u001b[0m (21.53 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,510</span> (21.53 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,508\u001b[0m (21.52 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,508</span> (21.52 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["# **License**"],"metadata":{"id":"QuxBiIDsUsNG"}},{"cell_type":"markdown","source":["This code is licensed under the MIT License.\n","\n","Author: Karman.\n","\n","Some Code Generated with AI.\n"],"metadata":{"id":"u-BDP1JBUoBk"}}]}